\section{Federal Websites}
\label{appendix:dotgov}

This appendix provides details about the Federal Websites dataset, which contains content from U.S. government websites in the .gov, .mil, and select other government-related domains. These websites represent official information from various federal agencies, departments, and institutions across all branches of government.

\subsection{Processing Statistics}

Table~\ref{table:dotgov-stats} shows the processing statistics for the Federal Websites dataset.

\begin{table}[h]
\centering
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{Processing Stage} & \textbf{Document Count} & \textbf{\% of Documents} & \textbf{\% of Initial} \\
\hline
Documents (Initial Collection) & 3,233,136 & 100\% & 100\% \\
Representations (Processed) & 3,191,427 & 98.7\% & 98.7\% \\
Parquet (Final Format) & 3,187,567 & 98.6\% & 98.6\% \\
\hline
\end{tabular}
\caption{Processing statistics for the Federal Websites dataset.}
\label{table:dotgov-stats}
\end{table}

The Federal Websites dataset demonstrates an excellent processing success rate of 98.6\% from initial collection to final format. This high rate indicates the quality and accessibility of government web content, with only minimal data loss during processing. The slight reduction between the Representations and Parquet stages (only 3,860 documents) further demonstrates the stability of the dataset through the processing pipeline.

\subsection{Domain Coverage}

The KL3M dataset includes content from hundreds of federal websites across multiple government branches and agencies. These domains can be organized into several major categories:

\begin{itemize}
  \item \textbf{Executive Branch Agencies} -- Includes websites from cabinet departments (e.g., www.dhs.gov, www.treasury.gov), independent agencies (e.g., www.epa.gov, www.nasa.gov), and their sub-agencies
  \item \textbf{Legislative Branch} -- Includes www.house.gov, www.senate.gov, www.gao.gov, www.cbo.gov
  \item \textbf{Judicial Branch} -- Includes court websites such as www.uscourts.gov and circuit courts
  \item \textbf{Military Domains} -- Various .mil domains including www.army.mil, www.navy.mil, www.af.mil
\end{itemize}

This broad coverage ensures representation from across the U.S. federal government, capturing the diversity of information produced by different agencies and institutions.

\subsection{Collection Methodology}

The Federal Websites dataset is collected through a comprehensive approach designed to capture diverse content types while filtering out non-informative materials. The collection methodology involves:

\begin{enumerate}
    \item \textbf{Source Selection} -- Carefully selecting government domains from the .gov and .mil TLDs, focusing on federal agencies
    
    \item \textbf{Content Filtering} -- Implementing sophisticated file type filtering that:
    \begin{itemize}
        \item Includes informative document formats like HTML, PDF, Word documents, Excel spreadsheets, PowerPoint presentations, XML, and plain text
        \item Excludes non-informative formats like images (PNG, JPG, GIF), style files (CSS), scripts (JS), and multimedia (MP3, MP4)
        \item Applies size thresholds to ensure document quality
    \end{itemize}
    
    \item \textbf{Metadata Extraction} -- For each document:
    \begin{itemize}
        \item Extracting basic metadata like title and description from HTML meta tags
        \item For PDFs, extracting embedded metadata and page counts using PyPDFium2
        \item Computing cryptographic hashes (blake2b) for content integrity verification
    \end{itemize}
    
    \item \textbf{Legal Compliance} -- Ensuring adherence to the public domain status of federal government works under 17 U.S.C. ยง 105, while filtering content that might have copyright restrictions
\end{enumerate}

This methodical approach ensures comprehensive coverage of valuable government information while maintaining data quality and legal compliance.

\subsection{Document Types and Content}

The Federal Websites dataset encompasses a wide variety of document types reflecting the diverse nature of government communications:

\begin{enumerate}
    \item \textbf{Web Pages (HTML/XHTML)}
    \begin{itemize}
        \item Agency information pages describing missions, services, and operations
        \item Public guidance and instructions for government services
        \item Press releases and news announcements
        \item Reports and publications in web format
    \end{itemize}
    
    \item \textbf{Document Files}
    \begin{itemize}
        \item PDFs of reports, studies, guidelines, and forms
        \item Word documents (.doc, .docx) with detailed information
        \item Spreadsheets (.xls, .xlsx) containing data and statistics
        \item Presentations (.ppt, .pptx) from government briefings
    \end{itemize}
    
    \item \textbf{Data Formats}
    \begin{itemize}
        \item XML files containing structured data
        \item CSV files with tabular data
        \item RSS and Atom feeds providing updates from agencies
    \end{itemize}
\end{enumerate}

The dataset specifically excludes non-informative content such as images, style sheets, scripts, and multimedia files to focus on textual and structured information.

\subsection{Technical Implementation}

The Federal Websites dataset collection is implemented using Python with several key technical components:

\begin{enumerate}
    \item \textbf{Content Type Detection}
    \begin{itemize}
        \item Using the \texttt{mimetypes} library to identify file types
        \item Implementing fallback mechanisms for ambiguous content types
        \item Maintaining allowlists (INCLUDE\_MIME\_TYPES) and blocklists (EXCLUDE\_EXTENSIONS) to filter content
    \end{itemize}
    
    \item \textbf{Metadata Extraction}
    \begin{itemize}
        \item Using regular expressions (HTML\_TITLE\_RE, HTML\_META\_RE) to extract HTML metadata
        \item Leveraging PyPDFium2 for PDF parsing and metadata extraction
        \item Normalizing inconsistent metadata fields (e.g., title vs. Title)
    \end{itemize}
    
    \item \textbf{Document Processing}
    \begin{itemize}
        \item Managing document identifiers based on source domain and path
        \item Handling various character encodings and format peculiarities
        \item Implementing error handling for malformed documents
    \end{itemize}
\end{enumerate}

This robust implementation ensures efficient processing of diverse government content while maintaining high data quality standards.

\subsection{Legal Status}

The Federal Websites dataset is primarily in the public domain under 17 U.S.C. ยง 105, which specifies that works created by the U.S. federal government are not subject to copyright protection. As stated in the dataset metadata: "Public domain under 17 U.S. Code ยง 105 after filtering copyrighted material and excluded agencies/GSEs."

The collection process includes filtering to exclude:

\begin{enumerate}
    \item Content from government-sponsored enterprises (GSEs) which may not be subject to the same public domain provisions
    \item Third-party copyrighted material that might be embedded in government websites
    \item Material from agencies explicitly excluded from the public domain provisions
\end{enumerate}

This careful filtering ensures the dataset's compliance with copyright law while maximizing the availability of valuable government information.

\subsection{Research Applications}

The Federal Websites dataset offers numerous research opportunities:

\begin{enumerate}
    \item \textbf{Government Operations Analysis}
    \begin{itemize}
        \item Studying how agencies communicate with the public
        \item Analyzing policy implementation across different agencies
        \item Tracking changes in government priorities over time
    \end{itemize}
    
    \item \textbf{Natural Language Processing}
    \begin{itemize}
        \item Training models on official government language and terminology
        \item Developing information extraction tools for government documents
        \item Creating summarization systems for lengthy government reports
    \end{itemize}
    
    \item \textbf{Public Information Studies}
    \begin{itemize}
        \item Measuring accessibility and readability of government communications
        \item Analyzing consistency of information across agencies
        \item Studying public data disclosure practices
    \end{itemize}
    
    \item \textbf{Digital Government Research}
    \begin{itemize}
        \item Evaluating e-government implementations
        \item Studying digital service delivery models
        \item Analyzing government website usability and information architecture
    \end{itemize}
\end{enumerate}

The dataset's comprehensive coverage of federal web content makes it valuable for researchers across multiple disciplines interested in government operations, communication, and information dissemination.