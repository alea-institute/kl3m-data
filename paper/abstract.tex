\begin{abstract}
{Practically all large language models have been pre-trained on data that is subject to global uncertainty related to copyright infringement, breach of contract, and privacy. In light of this reality, we present the 80TB+ sized Kelvin Legal Large Language Model (KL3M) dataset, the largest collections of pretraining and supervised fine-tuning data which is unencumbered by copyright risk.  In total, KL3M contains over 125 million documents and more than 1.7 trillion tokens.  This paper outlines our approach to creating a dataset free from copyright uncertainty including details on our data sources, and collection and preprocessing methods.  While our data sources are limited to US and certain EU source materials, we open source not only the dataset itself but also the tokenizer, API's and other associated software in order to support further allied development in other jurisdictions.  We believe that this represents a significant step towards developing A.I. systems that are legally and ethically sound, both now and in the face of future regulatory changes.}
\end{abstract}