\section{A Living Dataset and Infrastructure for the Collecting and Distributing Copyright Clean Data}
In this paper, we introduced the Kelvin Legal Large Language Model (KL3M) dataset a large and diverse corpora of more than 125 million documents and more than 1.7 trillion tokens.  We describe our copyright filtration process designed to identify only source materials with clear provenance from a copyright perspective.  We then provided an overview of the pre-processing pipeline designed to 





The KL3M can be used in several ways.  First, it can serve as a baseline for model pretraining and could be combined with other appropriately license datasets.  Alternatively, it could be used to fine tune an existing model.  We realize that this dataset alone would likely be insufficent to allow for models to be built which cover the boundless set of possible use cases and user queries.  However, we believe this large body of tokens could be combined with selected forms of licensed content to develop LLMs which can deliver strong performance on certain tasks.  However, 
We believe that the development of datasets and models that are transparent, freely available without legal restrictions, and high quality will enable downstream use that is free of the infringement concerns often present.


The paper reflects the current version of the KL3M dataset as of the time of this publication.  Yet, rather than being a static snapshot, we hope that KL3M will persist as ``living dataset'' which we seek to update, maintain and extend as time moves forward.  In addition, we hope that KL3M will become a federated project where others leverage or retrofit some of our underlying infrastructure to expand the set of copyright clean data available from a global perspective.  

The KL3M dataset represents a significant step towards creating large language model training data that is free from copyright uncertainty. By carefully selecting and vetting our sources, we have created a resource that can be used with confidence by AI researchers and developers. We hope that our methodology will serve as a template for future efforts in this direction, ultimately leading to more legally and ethically robust AI systems.

As the field of AI continues to evolve rapidly, it is crucial that we address the legal and ethical challenges head-on. The KL3M dataset is our contribution to this effort, providing a foundation for the development of AI systems that can withstand future regulatory scrutiny and contribute positively to society.  While our focus has been on US and EU jurisdictions due to our familiarity with their legal systems, we recognize the need for similar efforts in other parts of the world. We encourage researchers and legal experts from other jurisdictions to adapt and expand upon our methodology to create globally representative datasets that are free from copyright concerns.

In conclusion, the KL3M dataset not only provides valuable training data for large language models but also sets a new standard for transparency and legal compliance in AI development. As we move forward, it is our hope that this work will inspire further innovation in the responsible and ethical advancement of AI technology.



